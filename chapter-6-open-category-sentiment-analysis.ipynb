{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# Read dataset\nsample = pd.read_csv(\"../input/shopee-sentiment-analysis/sampleSubmission.csv\")\ntest_dataset = pd.read_csv(\"../input/shopee-sentiment-analysis/test.csv\")\ntrain_dataset = pd.read_csv(\"../input/shopee-sentiment-analysis/train.csv\")\n\n# Check dimension\nprint(\"Dimension Shape for test\", test_dataset.shape)\nprint(\"Dimension Shape for train\", train_dataset.shape)\nprint(\"Dimension Shape for sample\", sample.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Remove Punctuation\ntrain_dataset[\"review\"] = train_dataset['review'].str.replace('[?.!,Â¿()-/]',' \\1 ')\ntrain_dataset[\"review\"] = train_dataset['review'].str.replace('[^\\w\\s]',' ')\ntrain_dataset[\"review\"] = train_dataset['review'].str.replace('[\" \"]+', \" \")\n\ndisplay(train_dataset)\n\n# Stemming Process\nfrom nltk.stem.porter import PorterStemmer # Stemming\nfrom nltk.stem import WordNetLemmatizer\n\n# # Download Stopwords \nimport nltk\nnltk.download('stopwords')\n\n# # Allocate Stopwords to the variable\nfrom nltk.corpus import stopwords\nen_stops = list(stopwords.words('english'))\n\n# Plotting to the libraries\nlemmatizer = WordNetLemmatizer() # Lemmatization\nstemmer = PorterStemmer() # Stemming\n\n# Splitting the Sentence into words\ntrain_dataset[\"review\"] = train_dataset[\"review\"].str.split()\n\n# # # # Removing Stopwords\ntrain_dataset[\"review\"] = train_dataset[\"review\"].apply(lambda x: [word.lower() for word in x if word not in (en_stops)])\ndisplay(train_dataset[\"review\"])\n\n# Removing the Stemming \ntrain_dataset[\"review\"] = train_dataset[\"review\"].apply(lambda x: ([stemmer.stem(y) for y in x]))\n# Removing Lemmatization\ntrain_dataset[\"review\"] = train_dataset[\"review\"].apply(lambda x: ([lemmatizer.lemmatize(y) for y in x]))\n\ndisplay(train_dataset[\"review\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Train Test Split in Python in Training Only  \nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(train_dataset[\"review\"], train_dataset[\"rating\"], test_size = 0.3, random_state = 41)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Count Vectorizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.naive_bayes import BernoulliNB\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier\n\nX_train =[\" \".join(review) for review in X_train]\n\n# TFID Vectorizer\ncv = TfidfVectorizer()\ndf_xcv = cv.fit_transform(X_train)\n\n# Using Naive Bayes for the Model\nnaive_bayes = BernoulliNB()\nSVC = SVC(kernel = 'rbf', C = 1000, random_state = 0)\nclf = RandomForestClassifier(max_depth=20, random_state=0, criterion = 'entropy', n_estimators = 250)\n\n\n# Fitting into from X to Y\nnaive_bayes.fit(df_xcv, y_train)\nSVC.fit(df_xcv, y_train)\nclf.fit(df_xcv, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Splitting the Sentence into words\nX_test =[\" \".join(review) for review in X_test]\n\n# TFID Vectorizer\ndf_x_test = cv.transform(X_test)\ny_pred_naive = naive_bayes.predict(df_x_test)\ny_pred_svm = SVC.predict(df_x_test)\ny_pred_random = clf.predict(df_x_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Import Confusion Matrix \nfrom sklearn.metrics import confusion_matrix\nprint(\"Confusion Matrix as follows : \\n\")\nprint(confusion_matrix(y_test, y_pred_svm))\n\n# Import Accuracy Score \nfrom sklearn.metrics import accuracy_score, precision_score, recall_score\nprint(\"The Accuracy Score in Naive Bayes is : \", accuracy_score(y_test, y_pred_naive))\nprint(\"The Precision Score in Naive Bayes is : \", precision_score(y_test, y_pred_naive, average = 'weighted'))\nprint(\"The Recall Score in Naive Bayes is : \", recall_score(y_test, y_pred_naive,average = 'weighted'))\n\n\n# In the SVM \nprint(\"The Accuracy Score in SVM is : \", accuracy_score(y_test, y_pred_svm))\nprint(\"The Precision Score in SVM is : \", precision_score(y_test, y_pred_svm, average = 'weighted'))\nprint(\"The Recall Score in SVM is : \", recall_score(y_test, y_pred_svm,average = 'weighted'))\n\n\n# In the Random Forest\nprint(\"The Accuracy Score in Random Forest is : \", accuracy_score(y_test, y_pred_random))\nprint(\"The Precision Score in Random Forest is : \", precision_score(y_test, y_pred_random, average = 'weighted'))\nprint(\"The Recall Score in Random Forest is : \", recall_score(y_test, y_pred_random,average = 'weighted'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Target\ntest_dataset_count = cv.transform(test_dataset[\"review\"])\ntest_dataset[\"rating\"] = SVC.predict(test_dataset_count)\n\n# Export to a csv file\ntest_dataset.loc[:, ['review_id', 'rating']].to_csv(\n    'submission.csv', index=False, header=True\n)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Using Different Model Approach \n### Neural Network\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\ntokenizer = Tokenizer(num_words=5000)\ntokenizer.fit_on_texts(X_train)\n\nX_train = tokenizer.texts_to_sequences(X_train)\nX_test = tokenizer.texts_to_sequences(X_test)\n\nmaxlen = 100\nX_train = pad_sequences(X_train, padding='post', maxlen=maxlen)\nX_test = pad_sequences(X_test, padding='post', maxlen=maxlen)\n\nvocab_size = len(tokenizer.word_index) + 1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.utils import np_utils\n# Change the Format of Y train and Y test \ndummy_y_train = np_utils.to_categorical(y_train)\ndummy_y_test = np_utils.to_categorical(y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from tensorflow import keras\nfrom tensorflow.keras import layers\n\nembedding_dim = 50\nmodel = keras.Sequential()\n# Add an Embedding layer expecting input vocab of size 1000, and\n# output embedding dimension of size 64.\nmodel.add(layers.Embedding(input_dim=vocab_size, output_dim=embedding_dim))\n\n# Add a LSTM layer with 128 internal units.\nmodel.add(layers.Dense(100,activation = 'relu'))\n\n# Add Droping Out\nmodel.add(layers.Dropout(0.25))\n\n# Add a Dense layer with 10 units.\nmodel.add(layers.Dense(50,activation = 'softmax'))\n\n# Add Droping Out\nmodel.add(layers.Dropout(0.25))\n\n# Add a Dense layer with 10 units.\nmodel.add(layers.Dense(25,activation = 'softmax'))\n\n# Flatten the Model\nmodel.add(layers.Flatten())\n\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.compile(\n    loss=keras.losses.SparseCategoricalCrossentropy(),\n    optimizer=\"adam\",\n    metrics=[\"accuracy\"],\n)\n\nbatch_size = 64\nmodel.fit(\n    X_train, y_train, validation_data=(X_test, y_test), batch_size=batch_size, verbose=1, epochs=200\n)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}